{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lclarete/DHUM72500-FINAL-PORTFOLIO/blob/main/Clarete_week2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdt0sPtZxrO7"
      },
      "source": [
        "# Week 2: Getting Started with Jupyter and Google Collaboratory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXrIB5F5xrO9"
      },
      "source": [
        "# Introduction \n",
        "\n",
        "Each week, I'll ask you to fill out the information at the top of the notebook so that I can easily find your information and give you credit for the assignment. This year we will start using Google Collaboratory. There are several technologies stacked here, so it's ok if it feels confusing--that's because it is a little. \n",
        "\n",
        "First, Jupyter Notebooks (once called Python Notebooks, and I apologize if I slip when I'm referring to them sometimes) is a simplified, partially graphical way of running Python using the browser. It allows you to integrate markdown (or text) alongside code snippets, and the code can be executed directly in a browser window. That means that you're running a little Python kernal (like a mini server) in the background of your computer. Google Colab (or Collaboratory) takes Jupyter notebooks and expands on it, allowint you to work in a computational environment just like Jupyter notebooks, but using Google's computing power and in a way that can be directly saved like your any Google doc using Google Drive. \n",
        "\n",
        "# What to do\n",
        "\n",
        "Each week, you will find a link to the weekly notebook assignment on our course website. When you click on the link, it will take you to a file on GitHub. That file will have an icon at the top that says \"Open in Colab.\" Click on that link. That will open up the notebook in Google Colab. Before you do anything else, go to the menu at the top left of your window and clicek File --> Save a copy in Drive. You will see that the icon at the top left changes from the GitHub icon to the Google Drive icon, and the name of the file has changed to something starting with \"Copy of...\" From this point, follow these steps: \n",
        "\n",
        "* Change the name of the file to your LastName_Week#.ipynb. \n",
        "* Click File --> Move. \n",
        "* When the window to move your file opens, create a folder where you will save your weekly assignments, and then move the current file into that folder. I suggest that you name the folder something like Femethods_Weekly so you can find it easily. \n",
        "* Once your file is saved to your Google Drive folder, you will be able to make changes. Go to the top right hand corner where it says \"Connect\" and click on the drop down arrow. \n",
        "* Click on \"Connect to a hosted runtime.\" \n",
        "* Once your notebook is ready, it will have a green checkmark and two measuring bars, one for RAM and one for Disk. \n",
        "* Next, Click into the text cells below, and add your information. You can do this by hovering your mouse over a text cell and double clicking. \n",
        "\n",
        "Complete the notebook by reading each section. When you come to an executable cell (usually you can tell because those have a symbol like [ ] next to an area with gray highlighting and a different font with some code. Hover your mouse over the brackets, and you can click the \"play\" icon when it appears. Alternatively, you can click on the highlighted area. The cell will become editable. You can then press Control + Return (on Mac) or CTRL + Enter (on PC) and run the cell. When the cell runs, if there is an output, it will appear in the notebook directly below the code. \n",
        "\n",
        "Try it now and see how it goes..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Vp-hSaixrO-"
      },
      "source": [
        "* Student Name: Livia Clarete\n",
        "* Date: Feb 6 2023\n",
        "* Instructor: Lisa Rhody\n",
        "* Assignment due: Feb 6 2023\n",
        "* Methods of Text Analysis\n",
        "* MA in DH at The Graduate Center, CUNY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MV3brh3jxrO-"
      },
      "source": [
        "Click in the following cell and then click on the \"play\" icon to run it. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n_HIFVEgxrO-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7315f084-9ce0-4923-d68f-423a43ae84f2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "1 + 5 * 2 - 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "IeujceYExrPA"
      },
      "source": [
        "If the code is running correctly, a green check mark will appear to the left of the []. Inside the [] a number will appear. This number represents the \"process number\"--which is to say the order in which the code you executed was executed within the notebook. Also, you will see the result of the math equation below the cell where the equation is. The answer should be 8. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding a Markdown / Text cell\n",
        "\n",
        "At the top of your notebook and just under the menu (the list of words that begin File, Edit, View, etc...), there should be two actions you can perform. One is \"+ Code\" and the other is \"+ Text.\" These will allow you to add either a code cell or a text cell in the area beneath whichever cell is currently active. \n",
        "\n",
        "For your next trick, click on this cell to make it \"active.\" Then move your cursor to the top of the page and click the \"+Text\" action. A new cell should open up. Click inside the new cell and type an answer to one of the following questions: \n",
        "\n",
        "* What is your favorite comfort food? \n",
        "* What is your favorite book? \n",
        "* Do you have a pet? If so, what kind? and what is its name? "
      ],
      "metadata": {
        "id": "bS7YUloQFJBe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* What is your favorite comfort food? \n",
        "I'm currently in a croissant comfort food fase\n",
        "\n",
        "* What is your favorite book? \n",
        "The Tibetan Book of Living and Dying\n",
        "\n",
        "* Do you have a pet? If so, what kind? and what is its name? \n",
        "Both of my dogs died in the last months. Lola and Baby were 14 and 15 years old :("
      ],
      "metadata": {
        "id": "1aPltmoyIvPQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing NLTK and text functions\n",
        "\n",
        "Next, click in the cell below. This cell will tell the notebook to pull into its working memory a library called NLTK (the Natural Language Toolkit) which is something we will use quite a lot this semester. It will also download the book corpora. "
      ],
      "metadata": {
        "id": "1SUYTGC3GDS0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y3VxE8S1xrPA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15cfbbfa-f26a-48cf-f346-b1d21a1d5355"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'book'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection book\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import nltk \n",
        "nltk.download(\"book\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If this cell has executed properly, it will have lots of rows that begin [nltk_data] and at the end say \"True.\" We're not going to use much of NLTK this week, but it's helpful to practice because we'll need to do this a lot in the future. Please create a new text cell below. \n",
        "\n",
        "**EXERCISE**: If you had any problems, please explain. If not, just say \"All good.\" "
      ],
      "metadata": {
        "id": "MfhYAJPlGg3o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "All good."
      ],
      "metadata": {
        "id": "eixB50JXJXhX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMgYoXntxrPB"
      },
      "source": [
        "Finally, we're just going to play around with what we installed a little bit to make sure it's working for you. No worries if you have trouble with it... that's why we're doing this. Just to practice. \n",
        "\n",
        "**EXERCISE Part 1**: First, run the cell below. This is a sentence taken from the following article: Fabricant, Florence. “Porcini Mushrooms for Those With Fungi on the Brain.” The New York Times, 30 Jan. 2023. NYTimes.com, https://www.nytimes.com/2023/01/30/dining/mushrooms-porcini-italian.html.\n",
        "\n",
        "The output should include 2 rows. The first row will cut the text up into units called \"tokens.\" The second row will tag each word with a label for the part of speech. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LOsAFnBCxrPB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eacebae8-72c2-47c3-d6f8-fe4413aa331f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['We', 'begin', 'our', 'exploration', 'with', 'the', 'identification', 'of', 'research', 'questions', ',', 'proceed', 'through', 'data', 'selection', ',', 'conceptualization', ',', 'and', 'operationalization', ',', 'and', 'end', 'with', 'analysis', 'and', 'the', 'interpretation', 'of', 'results']\n",
            "[('We', 'PRP'), ('begin', 'VBP'), ('our', 'PRP$'), ('exploration', 'NN'), ('with', 'IN'), ('the', 'DT'), ('identification', 'NN'), ('of', 'IN'), ('research', 'NN'), ('questions', 'NNS'), (',', ','), ('proceed', 'VBP'), ('through', 'IN'), ('data', 'NNS'), ('selection', 'NN'), (',', ','), ('conceptualization', 'NN'), (',', ','), ('and', 'CC'), ('operationalization', 'NN'), (',', ','), ('and', 'CC'), ('end', 'NN'), ('with', 'IN'), ('analysis', 'NN'), ('and', 'CC'), ('the', 'DT'), ('interpretation', 'NN'), ('of', 'IN'), ('results', 'NNS')]\n"
          ]
        }
      ],
      "source": [
        "sentence = \"\"\"We begin our exploration with the identification of research questions,\n",
        "proceed through data selection, conceptualization, and operationalization, and\n",
        "end with analysis and the interpretation of results\"\"\"\n",
        "\n",
        "tokens = nltk.word_tokenize(sentence)\n",
        "\n",
        "print(tokens)\n",
        "\n",
        "tagged = nltk.pos_tag(tokens)\n",
        "\n",
        "print(tagged)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EXERCISE Part Two** Click into the code cell above and replace the sentence beginning with Porcini all the way to the word purveyors with your own sentence or two. Be sure to leave in all 3 sets of quotation marks. Then run the code. "
      ],
      "metadata": {
        "id": "y3Hx7KhxIIuU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRQv5b78xrPB"
      },
      "source": [
        "# Congratulations!!! You've completed your first assignment. \n",
        "\n",
        "Click File, and in the drop down menu, select \"Save.\" Make sure that your notebook file saves in a folder that is shared with my gmail address: lisarhody.gc@gmail.com. \n",
        "\n",
        "If you are someone who would like to push further, You could get started with the NLTK Book Chapter 1 here: https://www.nltk.org/book/ch01.html. You can create a new notebook to track your work and submit that the same way. If you do, please save the notebook with the same naming strategy: lastname-week.ipynb. \n",
        "\n",
        "Congratulations! You're well on your way!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}